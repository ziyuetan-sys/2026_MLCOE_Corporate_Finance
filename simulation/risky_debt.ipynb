{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dab38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-31 04:04:16.187488: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-31 04:04:17.540931: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\")) \n",
    "from dynamic_model.Debt import RiskFree, RiskDebt\n",
    "from dynamic_model.Tetworks import BellmanNet_RiskyFree, BellmanNet_RiskDebt\n",
    "from dynamic_modelTrainer import BellmanTrainer\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bafbf857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-31 04:04:22.188561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14633 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:61:00.0, compute capability: 7.0\n",
      "2025-12-31 04:04:22.189299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14633 MB memory:  -> device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:62:00.0, compute capability: 7.0\n",
      "2025-12-31 04:04:22.189971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14634 MB memory:  -> device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:89:00.0, compute capability: 7.0\n",
      "2025-12-31 04:04:22.190546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 14634 MB memory:  -> device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:8a:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "model  = RiskFree()\n",
    "net =  BellmanNet_RiskyFree(model)\n",
    "\n",
    "trainer = BellmanTrainer(\n",
    "    model=model,\n",
    "    net=net,\n",
    "    batch_size=64,\n",
    "    lr= 1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d293f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:   0%|                                                                              | 0/2000 [00:00<?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-31 04:04:22.626414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network weights initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-31 04:04:28.349355: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xbd133240 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-12-31 04:04:28.349382: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2025-12-31 04:04:28.349389: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2025-12-31 04:04:28.349394: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2025-12-31 04:04:28.349398: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2025-12-31 04:04:28.356958: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-12-31 04:04:28.553861: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Training progress:   0%|                                                                              | 0/2000 [00:08<?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     1 | loss_total=4.601257e+05 | loss_V=4.587956e+05 | loss_FOC_K=1.111989e+00 | loss_FB=3.554925e-03\n",
      " Eval @ epoch 1: lifetime reward = 7.022e+02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  25%|█████████████████▉                                                      | 497/2000 [00:25<00:42]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   500 | loss_total=4.546812e+02 | loss_V=4.525729e+02 | loss_FOC_K=1.026565e-01 | loss_FB=0.000000e+00\n",
      " Eval @ epoch 500: lifetime reward = 7.021e+02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  50%|███████████████████████████████████▉                                    | 998/2000 [00:42<00:28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1000 | loss_total=3.283138e+02 | loss_V=3.258651e+02 | loss_FOC_K=4.883508e-04 | loss_FB=0.000000e+00\n",
      " Eval @ epoch 1000: lifetime reward = 7.022e+02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  75%|█████████████████████████████████████████████████████▏                 | 1497/2000 [00:58<00:14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1500 | loss_total=3.596898e+02 | loss_V=3.567528e+02 | loss_FOC_K=1.337065e-03 | loss_FB=0.000000e+00\n",
      " Eval @ epoch 1500: lifetime reward = 7.022e+02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress: 100%|██████████████████████████████████████████████████████████████████████▊| 1996/2000 [01:14<00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2000 | loss_total=3.992848e+02 | loss_V=3.963669e+02 | loss_FOC_K=6.975376e-04 | loss_FB=0.000000e+00\n",
      " Eval @ epoch 2000: lifetime reward = 7.022e+02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress: 100%|███████████████████████████████████████████████████████████████████████| 2000/2000 [01:14<00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n",
      "Best reward: 7.026e+02 @ epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(training_steps = 2000,  display_step= 500, early_stop = False)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "346affb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def r_tilde_diff(\n",
    "    r_old, r_new, \n",
    "    tol_default=0.0,\n",
    "    cross_penalty=1.0    # 现在用于显示分类占比，不放大差异\n",
    "):\n",
    "    default_old = r_old > 1e7\n",
    "    default_new = r_new > 1e7\n",
    "\n",
    "    mask_both_ok = tf.logical_and(~default_old, ~default_new) # No default for both\n",
    "    mask_cross = tf.not_equal(default_old, default_new) # One default, one not\n",
    "    mask_both_default = tf.logical_and(default_old, default_new) # Both default\n",
    "\n",
    "\n",
    "    diff_normal = tf.abs(r_new - r_old)\n",
    "    diff_cross  = cross_penalty  * tf.ones_like(r_old, dtype=tf.float32) \n",
    "    diff_both   = tf.zeros_like(r_old)\n",
    "\n",
    "    diff_map = (\n",
    "        tf.where(mask_both_ok, diff_normal, tf.zeros_like(diff_normal)) +\n",
    "        tf.where(mask_cross, diff_cross, tf.zeros_like(diff_cross)) +\n",
    "        tf.where(mask_both_default, diff_both, tf.zeros_like(diff_both))\n",
    "    )\n",
    "\n",
    "  \n",
    "    N_total = tf.cast(tf.size(r_old), tf.float32)\n",
    "    num_ok = tf.reduce_sum(tf.cast(mask_both_ok, tf.float32))\n",
    "    num_cross = tf.reduce_sum(tf.cast(mask_cross, tf.float32))\n",
    "    num_default = tf.reduce_sum(tf.cast(mask_both_default, tf.float32))\n",
    "\n",
    "    ratio_ok = num_ok / N_total * 100\n",
    "    ratio_cross = num_cross / N_total  * 100\n",
    "    ratio_default = num_default / N_total * 100\n",
    "\n",
    "    diff_mean = tf.reduce_mean(diff_map)\n",
    "        \n",
    "    tf.print(\n",
    "        \"both_ok: %.2f%%\" % ratio_ok,\n",
    "        \"cross: %.2f%%\" % ratio_cross,\n",
    "        \"both_default: %.2f%%\" % ratio_default\n",
    "    )\n",
    "    all_default = tf.equal(num_default, N_total)\n",
    "    if all_default:\n",
    "        tf.print(\" All states in default, not counted as convergence.\")\n",
    "        diff_mean = tf.constant(1e6, dtype=tf.float32)\n",
    "\n",
    "    return diff_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7179d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "riskfree_model = model\n",
    "riskfree_net = net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec0e2ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RiskDebt_model = RiskDebt(prev_net= riskfree_net)\n",
    "riskdebt_net = BellmanNet_RiskDebt(RiskDebt_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "420102e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Outer iteration 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:   0%|                                                                              | 0/2000 [00:00<?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training ...\n",
      "Network weights initialized.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bellman_net__risky_free/layer_normalization/gamma:0', 'bellman_net__risky_free/layer_normalization/beta:0', 'bellman_net__risky_free/dense/kernel:0', 'bellman_net__risky_free/dense/bias:0', 'bellman_net__risky_free/dense_1/kernel:0', 'bellman_net__risky_free/dense_1/bias:0', 'bellman_net__risky_free/dense_2/kernel:0', 'bellman_net__risky_free/dense_2/bias:0', 'bellman_net__risky_free/dense_3/kernel:0', 'bellman_net__risky_free/dense_3/bias:0', 'bellman_net__risky_free/dense_4/kernel:0', 'bellman_net__risky_free/dense_4/bias:0', 'bellman_net__risky_free/dense_5/kernel:0', 'bellman_net__risky_free/dense_5/bias:0', 'bellman_net__risky_free/dense_6/kernel:0', 'bellman_net__risky_free/dense_6/bias:0', 'bellman_net__risky_free/dense_7/kernel:0', 'bellman_net__risky_free/dense_7/bias:0', 'bellman_net__risky_free/dense_8/kernel:0', 'bellman_net__risky_free/dense_8/bias:0', 'bellman_net__risky_free/dense_9/kernel:0', 'bellman_net__risky_free/dense_9/bias:0', 'bellman_net__risky_free/dense_10/kernel:0', 'bellman_net__risky_free/dense_10/bias:0', 'bellman_net__risky_free/dense_11/kernel:0', 'bellman_net__risky_free/dense_11/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bellman_net__risky_free/layer_normalization/gamma:0', 'bellman_net__risky_free/layer_normalization/beta:0', 'bellman_net__risky_free/dense/kernel:0', 'bellman_net__risky_free/dense/bias:0', 'bellman_net__risky_free/dense_1/kernel:0', 'bellman_net__risky_free/dense_1/bias:0', 'bellman_net__risky_free/dense_2/kernel:0', 'bellman_net__risky_free/dense_2/bias:0', 'bellman_net__risky_free/dense_3/kernel:0', 'bellman_net__risky_free/dense_3/bias:0', 'bellman_net__risky_free/dense_4/kernel:0', 'bellman_net__risky_free/dense_4/bias:0', 'bellman_net__risky_free/dense_5/kernel:0', 'bellman_net__risky_free/dense_5/bias:0', 'bellman_net__risky_free/dense_6/kernel:0', 'bellman_net__risky_free/dense_6/bias:0', 'bellman_net__risky_free/dense_7/kernel:0', 'bellman_net__risky_free/dense_7/bias:0', 'bellman_net__risky_free/dense_8/kernel:0', 'bellman_net__risky_free/dense_8/bias:0', 'bellman_net__risky_free/dense_9/kernel:0', 'bellman_net__risky_free/dense_9/bias:0', 'bellman_net__risky_free/dense_10/kernel:0', 'bellman_net__risky_free/dense_10/bias:0', 'bellman_net__risky_free/dense_11/kernel:0', 'bellman_net__risky_free/dense_11/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bellman_net__risky_free/layer_normalization/gamma:0', 'bellman_net__risky_free/layer_normalization/beta:0', 'bellman_net__risky_free/dense/kernel:0', 'bellman_net__risky_free/dense/bias:0', 'bellman_net__risky_free/dense_1/kernel:0', 'bellman_net__risky_free/dense_1/bias:0', 'bellman_net__risky_free/dense_2/kernel:0', 'bellman_net__risky_free/dense_2/bias:0', 'bellman_net__risky_free/dense_3/kernel:0', 'bellman_net__risky_free/dense_3/bias:0', 'bellman_net__risky_free/dense_4/kernel:0', 'bellman_net__risky_free/dense_4/bias:0', 'bellman_net__risky_free/dense_5/kernel:0', 'bellman_net__risky_free/dense_5/bias:0', 'bellman_net__risky_free/dense_6/kernel:0', 'bellman_net__risky_free/dense_6/bias:0', 'bellman_net__risky_free/dense_7/kernel:0', 'bellman_net__risky_free/dense_7/bias:0', 'bellman_net__risky_free/dense_8/kernel:0', 'bellman_net__risky_free/dense_8/bias:0', 'bellman_net__risky_free/dense_9/kernel:0', 'bellman_net__risky_free/dense_9/bias:0', 'bellman_net__risky_free/dense_10/kernel:0', 'bellman_net__risky_free/dense_10/bias:0', 'bellman_net__risky_free/dense_11/kernel:0', 'bellman_net__risky_free/dense_11/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bellman_net__risky_free/layer_normalization/gamma:0', 'bellman_net__risky_free/layer_normalization/beta:0', 'bellman_net__risky_free/dense/kernel:0', 'bellman_net__risky_free/dense/bias:0', 'bellman_net__risky_free/dense_1/kernel:0', 'bellman_net__risky_free/dense_1/bias:0', 'bellman_net__risky_free/dense_2/kernel:0', 'bellman_net__risky_free/dense_2/bias:0', 'bellman_net__risky_free/dense_3/kernel:0', 'bellman_net__risky_free/dense_3/bias:0', 'bellman_net__risky_free/dense_4/kernel:0', 'bellman_net__risky_free/dense_4/bias:0', 'bellman_net__risky_free/dense_5/kernel:0', 'bellman_net__risky_free/dense_5/bias:0', 'bellman_net__risky_free/dense_6/kernel:0', 'bellman_net__risky_free/dense_6/bias:0', 'bellman_net__risky_free/dense_7/kernel:0', 'bellman_net__risky_free/dense_7/bias:0', 'bellman_net__risky_free/dense_8/kernel:0', 'bellman_net__risky_free/dense_8/bias:0', 'bellman_net__risky_free/dense_9/kernel:0', 'bellman_net__risky_free/dense_9/bias:0', 'bellman_net__risky_free/dense_10/kernel:0', 'bellman_net__risky_free/dense_10/bias:0', 'bellman_net__risky_free/dense_11/kernel:0', 'bellman_net__risky_free/dense_11/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:   0%|                                                                              | 0/2000 [00:05<?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     1 | loss_total=5.054780e+05 | loss_V=5.054568e+05 | loss_FOC_K=1.123483e+00 | loss_FOC_B=9.980808e-01\n",
      " Eval @ epoch 1: lifetime reward = 7.156e+02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  11%|███████▋                                                                | 215/2000 [00:14<01:11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overfitting detected at epoch 220. Best reward = 1.0336e+03 (epoch 170)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:  11%|███████▉                                                                | 219/2000 [00:14<01:58]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n",
      "Best reward: 1.034e+03 @ epoch 170\n",
      "Training stopped early (overfitting at epoch 220). Restored best model from epoch 170.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both_ok: 100.00% cross: 0.00% both_default: 0.00%\n",
      "Mean weighted abs diff of r_tilde (incl. default): 1.86264515e-08\n",
      " Converged after 1 outer iterations. Stop training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_size     = 64\n",
    "learning_rate  = 1e-4\n",
    "max_outer_iter = 20          \n",
    "max_inner_iter = 2000        \n",
    "tol_rdiff      = 0.05     \n",
    "\n",
    "\n",
    "test_data = RiskDebt_model.sample_state_test(10000) \n",
    "K, B, Z = test_data\n",
    "prev_net = riskfree_net\n",
    "r_tilde_old = tf.ones((K.shape[0], 1), dtype=tf.float32) * model.r \n",
    "\n",
    "for outer in range(max_outer_iter):\n",
    "\n",
    "    print(f\"\\n Outer iteration {outer+1}/{max_outer_iter}\")\n",
    "\n",
    "    riskdebt_model = RiskDebt(prev_net=prev_net, alpha=0.1)\n",
    "    riskdebt_net   = BellmanNet_RiskDebt(riskdebt_model)\n",
    "\n",
    "    # inner loop trainer\n",
    "    trainer = BellmanTrainer(\n",
    "        model=riskdebt_model,\n",
    "        net=riskdebt_net,\n",
    "        train_data=None,\n",
    "        batch_size=batch_size,\n",
    "        lr=learning_rate\n",
    "    )\n",
    "    \n",
    "    trainer.fit(\n",
    "        training_steps=max_inner_iter,\n",
    "        display_step = 500,\n",
    "        eval=True,\n",
    "        eval_interval=5,\n",
    "        early_stop= True,  \n",
    "        reward_drop_tolerance= 10,\n",
    "        n_eval_points= 2048\n",
    "    )\n",
    "\n",
    "    r_tilde_new = riskdebt_model.get_r_tilde_test(riskdebt_net, test_data)\n",
    "    Value_new = riskdebt_net.value(K, B, Z)\n",
    "    \n",
    "    diff_r_mean = r_tilde_diff(r_tilde_old, r_tilde_new)\n",
    "    tf.print(f\"Mean weighted abs diff of r_tilde (incl. default):\", diff_r_mean)\n",
    "    #print(f\"Mean abs diff between r_tilde: {diff_r_mean:.6f}\")\n",
    "    r_tilde_old = r_tilde_new\n",
    "  \n",
    "    if diff_r_mean < tol_rdiff:\n",
    "        print(f\" Converged after {outer+1} outer iterations. Stop training.\")\n",
    "        break\n",
    "\n",
    "    prev_net = riskdebt_net\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c597ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
